---
title: Data mining of CNC data for cutting tool wear identification
author:
    - "JosÃ© Luis PÃ©rez"
    - "Fernando Lango"
    - "Thomas M. Rudolf"
include-in-header:
  - text: |
      <style>
      #title-slide .title {
        font-size: 1.5em;
        margin-bottom: 10%;
      }
      p, li {
        font-size: 16pt;
      }
      .reveal .footer {
        z-index: 0; 
      }
      .reveal .slide figure > figcaption,
      .reveal .slide img.stretch + p.caption,
      .reveal .slide img.r-stretch + p.caption {
        font-size: 12px;
      }
      </style>
format: 
    revealjs:
        theme: serif
        logo: images/logo-ITAM.png
        footer: "MinerÃ­a y AnÃ¡lisis de Datos"
---

```{r}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, cache.lazy = FALSE)

library(tidyverse)
library(kableExtra)
library(skimr)
library(corrplot)
library(factoextra)

options(scipen = 999)
```

## Problem

Identify the characteristics and CNC signals that help to indicate tool wear. The general behaviour of wear is illustrated in fig. 1a. At the beginning, a new component (in this case a cutting tool) has a progressive, non linear phase, where the required current to cut increases slowly (1). The following phase has a linear behaviour with very little increase under the same cutting conditions (2) until it reaches the end of usage (3). In this last phase a very progressive increase of the required current can be noticed.

![fig. 1: Wear curve of a cutting tool](images/clipboard-1073081830.png)

## Problem

Fig.2 shows the measurement of tool wear taken form different stage of the cutting process with reference to removed volume.

![fig. 2: measurements of tool wear per volume](images/ToolWear_with_volume_ref.png)

The challenge is to identify which of the available signals are the best indicator to identify a worn tool.

## Project Structure

![Fig. 3: Project Structure](images/project_structure.png)

## Model

::::: columns
::: {.column width="50%"}
![fig. 4: Indicator calculated form cutting model and real data](images/objective.png)
:::

::: {.column width="50%"}
There are different models that describe the cutting force $F_c$ and the resulting cutting torque $M_c$ . The one used in this project is the model proposed by Otto Kienzle. It is based on the volume that each cutting tool removes from the material and two material parameters the specific cutting force $k_{c1,1}$ \[units $N/m^2$\] and increasing value of the specific cutting force $1-m_c$ . The parameters ğ‘ and â„ are the geometric values of the removed material.

It is necessary to estimate accurately the value for both parameter to model relaiable the cutting torque, since small variations have a significant impact of the modelled signal.
:::
:::::

## Data structure

The available data are stored in folder with the structure illustrated in the fig. 5. The meta data of each measurement is stored in the folders name {cutting edge type}\_{number of cutting edges}\_{cutting depth in mm}\_{tool diameter}\_{% of tool diameter involved}\_{process}

![fig. 5: Data structure](images/DataStructure.png)

Each csv file is a time series with a duration of approximately $10$ seconds and a sample time of $2 ms$, leading to approximatly $5,000$ entries per variable per csv file.

## Selected data {.smaller}

```{r}
directory_path <- "data/"
csv_files <- list.files(path = directory_path, pattern = "\\.csv$", full.names = TRUE, recursive = TRUE)

column_mapping <- c(
  'time' = 'time',
  '+/Nck/!SD/nckServoDataActCurr32 [u1; 1]' = 'torque_x',
  '+/Nck/!SD/nckServoDataActCurr32 [u1; 2]' = 'torque_y',
  '+/Nck/!SD/nckServoDataActCurr32 [u1; 3]' = 'torque_z',
  '+/Nck/!SD/nckServoDataActCurr32 [u1; 4]' = 'torque_spindle',
  #'+/Nck/!SD/nckServoDataActPos2ndEnc32 [u1; 1]' = 'posAx1',
  #'+/Nck/!SD/nckServoDataActPos2ndEnc32 [u1; 2]' = 'posAx2',
  #'+/Nck/!SD/nckServoDataActPos2ndEnc32 [u1; 3]' = 'posAx3',
  #'+/Nck/!SD/nckServoDataActPower32 [u1; 6]' = 'PowerSp',
  #'+/Nck/!SD/nckServoDataActVelMot32 [u1; 6]' = 'VelMotSp',
  #'+/Nck/!SD/nckServoDataActVelMot32 [u1; 4]' = 'ActVelMot32',
  '+/Nck/!SD/nckServoDataActPos1stEnc32 [u1; 1]' = 'position_x',
  '+/Nck/!SD/nckServoDataActPos1stEnc32 [u1; 2]' = 'position_y',
  '+/Nck/!SD/nckServoDataActPos1stEnc32 [u1; 3]' = 'position_z',
  '+/Nck/!SD/nckServoDataActPos1stEnc32 [u1; 4]' = 'position_spindle',    
  '+/Channel/!RP/rpa [u1; 15]' = 'Recording_flag'
)

data <- list()

for (file in csv_files){
    df <- read_csv(file, col_names = TRUE, show_col_types = FALSE)
    # Obtener nombres de columnas presentes en el archivo y que estÃ¡n en el diccionario
    columns_to_rename <- intersect(names(df), names(column_mapping))
    
    # Renombrar las columnas solo si coinciden con el diccionario
    if (length(columns_to_rename) > 0) {
        df <- df %>%
            rename_with(~ column_mapping[.x], .cols = columns_to_rename)
    }
    
    df$file_path <- file
    file_path_parts <- str_split(file, "/")[[1]]
    
    df$file_name <- file_path_parts[length(file_path_parts)]
    df$file_dir <- file_path_parts[length(file_path_parts) - 1]
    
    df$time_increment <- c(0, diff(df$time))
    data[[file]] <- df
}

selected_columns <- c(unname(column_mapping), "file_path", "file_dir",
                      "file_name", "time_increment")


combined_data <- bind_rows(data) %>%
    select(any_of(selected_columns))
```

The data we will use in the analysis has been limited to the *WearMillingY_50* process for the CNC E900, as we determined that this process provides the best data quality.

In total, we have `r length(csv_files)` CSV files containing sensor signals. These files represent a total of `r nrow(combined_data)` observations and `r length(column_mapping)` columns:

```{r}
combined_data %>%
    as.data.frame() %>%
    str()
```

## Missing data

For variables with 7,953 missing data points, the records will be removed, as they belong to a single erroneous CSV file.

Variables with a large amount of missing data will be removed.

```{r}
data.frame(
  Missing = colSums(is.na(combined_data)),
  Percent = round(colSums(is.na(combined_data))/nrow(combined_data)*100, 2)) %>%
    kable() %>%
    kable_styling(font_size = 16)
```

```{r}
clean_data <-
    combined_data %>%
    select(-c("torque_z", "position_x", "Recording_flag")) %>%
    drop_na()
```

## Data cleaning

Most of the folders with process data follow a structure like "Level\_###," where \### is a number. We identified some with a different structure, which correspond to temporary files, other processes, or instances where recording did not function correctly. Therefore, they will be removed.

```{r}
data.frame(
    file_dir = unique(clean_data$file_dir)
) %>%
    filter(!grepl("^Level_[0-9]{3}$", file_dir))  %>%
    kable() %>%
    kable_styling(font_size = 20)
    
```

```{r}
clean_data <-
    clean_data %>%
    filter(grepl("^Level_[0-9]{3}$", file_dir)) %>%
    arrange(file_dir, file_name, time)

clean_data <-
    clean_data %>%
    group_by(file_dir) %>%
    mutate(level_time = cumsum(time_increment)) %>%
    ungroup() %>%
    mutate(total_time = cumsum(time_increment))
```

Resulting in a total of `r nrow(clean_data)` observations.

## Data cleaning

Additionally, we have created the variables level_time and total_time to track the total elapsed time of the process and for each level.

::::: columns
::: {.column width="50%"}
```{r}
clean_data %>%
    select(file_dir, level_time, total_time) %>%
    as.data.frame() %>%
    head() %>%
    kable() %>%
    kable_styling(font_size = 20)
```
:::

::: {.column width="50%"}
```{r}
clean_data %>%
    select(file_dir, level_time, total_time) %>%
    mutate(total_time = sprintf("%.3f", total_time)) %>%
    as.data.frame() %>%
    tail() %>%
    kable() %>%
    kable_styling(font_size = 20)
```
:::
:::::

## Statistical summary

With our data cleaned, we can observe a statistical summary of the numerical variables.

```{r}
stats <-
    clean_data %>%
    select(c("torque_x", "torque_y", "torque_spindle", "position_y",
           "position_z", "position_spindle")) %>%
    skim()

stats %>%
    select(skim_variable, starts_with("numeric.")) %>%
    mutate_if(is.numeric, round, 2) %>%
    rename_with(~ gsub("^numeric\\.", "", .), starts_with("numeric.")) %>%
    rename(variable = skim_variable) %>%
    kable() %>%
    kable_styling(font_size = 20)
```

## Correlations & PCA

```{r}
clean_data %>%
    select(c("torque_x", "torque_y", "torque_spindle",
             "position_y", "position_z", "position_spindle")) %>%
    cor() %>%
    corrplot(method = "number", type = "upper", tl.col = "black", tl.srt = 45)
```

## Correlations & PCA {.scrollable}

::::: columns
::: {.column width="50%"}
```{r}
clean_data %>%
    select(c("torque_x", "torque_y", "torque_spindle",
             "position_y", "position_z", "position_spindle")) %>%
    prcomp(scale = TRUE) %>%
    fviz_pca_var(col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE,
                 title = "PCA - .002 s")
```
:::

::: {.column width="50%"}
```{r}
clean_data %>%
    select(c("torque_x", "torque_y", "torque_spindle",
             "position_y", "position_z", "position_spindle")) %>%
    prcomp(scale = TRUE) %>%
    fviz_contrib(choice = "var", axes = 1, top = 10, title = "PCA - .002 s")
```
:::
:::::

::::: columns
::: {.column width="50%"}
```{r}
clean_data %>%
    filter(level_time %% .008 == 0) %>%
    select(c("torque_x", "torque_y", "torque_spindle",
             "position_y", "position_z", "position_spindle")) %>%
    prcomp(scale = TRUE) %>%
    fviz_pca_var(col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE,
                 title = "PCA - .008 s")
```
:::

::: {.column width="50%"}
```{r}
clean_data %>%
    filter(level_time %% .008 == 0) %>%
    select(c("torque_x", "torque_y", "torque_spindle",
             "position_y", "position_z", "position_spindle")) %>%
    prcomp(scale = TRUE) %>%
    fviz_contrib(choice = "var", axes = 1, top = 10, title = "PCA - .008 s")
```
:::
:::::

::::: columns
::: {.column width="50%"}
```{r}
clean_data %>%
    filter(level_time %% .016 == 0) %>%
    select(c("torque_x", "torque_y", "torque_spindle",
             "position_y", "position_z", "position_spindle")) %>%
    prcomp(scale = TRUE) %>%
    fviz_pca_var(col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE,
                 title = "PCA - .016 s")
```
:::

::: {.column width="50%"}
```{r}
clean_data %>%
    filter(level_time %% .016 == 0) %>%
    select(c("torque_x", "torque_y", "torque_spindle",
             "position_y", "position_z", "position_spindle")) %>%
    prcomp(scale = TRUE) %>%
    fviz_contrib(choice = "var", axes = 1, top = 10, title = "PCA - .016 s")
```
:::
:::::

## Signal processing on the current signals

In order to start analysing the most highly appreciated signals, some signal processing methods were conducted:

1.  Moving Standard Scalar: $x_{ssc}=\frac{x-\mu_{mov}}{\sigma_{mov}}$

With $x$ the current value, $\mu_{mov}$ the moving average value of the last N values and $\sigma_{mov}$ the standard deviation over the last N values.

2.  low pass Butterworth filter: This filter cuts values with frequencies higher as a defined frequency form the signal and makes it "cleaner".

## Signal processing on the current signals

::::: columns
::: {.column width="50%"}
Applying the moving standard scalar to the signal of the spindle current eliminates the static component to the signal and leaves the dynamic of a signal. This is helpful in this application since we are analysing the amplitudes resulting form the material removing process and we do not want static behaviour in our signal. The following figure illustrates the results. The green signal is the rough signal without any precessing. The blue and orange signals are the butterworth filtered signal and the moving standard scalar signal, respectively. The same function calculating the standard scalar value returns the moving average value (not illustrated) to have the static behaviour as well.
:::

::: {.column width="50%"}
![](images/clipboard-2041312673.png)
:::
:::::

## Fast Fourier Transformation (FFT)

Over the moving standard scalar value, the FFT was applied. Since the static components were eliminated, the the FFT has two significant peaks. The first one at aprox. $35Hz$ and the $105Hz$. $35Hz$ corresponds to the rotational speed and $105Hz$ is a multiple of the spindle speed and corresponds to the 3 cutting edges used. Doing the FFT over the complete time series gives the relevant information about significant frequencies. However, a FFT over a defined number of samples can lead to more information about when the material removing process starts and ends. To identify the amplitudes of the cutting signals, this is an important indicator.

![](images/clipboard-3589973023.png){fig-align="center"}

For this reason, the next step is to divide the time series in time windows and calculate the FFT again for each window. By doing this, one obtains a "moving FFT" with corresponding information.

## Window FFT (wFFT) 

For the wFFT, the same function was used as for the complete FFT. The algorithm starts evaluating the first N samples ($1....N$) and calculates the FFT over these signals. In the next step, it takes the values form $2...N+1$ and so on. Fig ? the results. For illustrative purposes, only some wFFTs are ploted (each 1000 times in this case.) What can be noticed that both frequencies are always dominant. The the frequency at $105Hz$, however, increases significantly when the material removing process starts and decreases a when finished

::::: columns
::: {.column width="45%"}
![window FFT over the complete time series (examples each 1000 FFT)](images/wFFT.png)
:::

::: {.column width="55%"}
![examples for wFFT at the beginning and during material removing process.](images/clipboard-1940175733.png)
:::
:::::

## Model: Gibbs Sampler (Jags model)

As a first approach to find reasonable values for $m_c$ and $k_{c,11}$ a gibbs sampler using jags was implemented using as input the maximal values of

$$
\hat{M_c} = \sum_1^z{F_{ci} r_{tool}}=a_p f_z^{1-m_c} sin(\kappa)^{m_c} k_{c1.1}
$$

$$
\tau \sim \gamma(0.1,0.1)
$$

$$
\hat{M_c} \sim dnorm(predict[i], \tau)
$$

$$
logâ¡(ğ‘ğ‘Ÿğ‘’ğ‘‘[ğ‘–])= logâ¡(a_p) + (1 âˆ’ ğ‘š_ğ‘)âˆ— logâ¡(ğ‘“_ğ‘§) + (1âˆ’ğ‘š_ğ‘)âˆ—logâ¡(sinâ¡(ğœ…)) +logâ¡(ğ‘˜_{ğ‘11}) +logâ¡( ğ‘Ÿ_{ğ‘¡ğ‘œğ‘œğ‘™})
$$

## Model: Gibbs Sampler (Jags model)

::::: columns
::: {.column width="50%"}
![result gibbs sampler $k_{c11}$](images/gibbs_sampler_kc.png)
:::

::: {.column width="50%"}
![results gibbs sampler $m_c$](images/gibbs_sampler_mc.png)
:::
:::::

The first results for $ğ‘˜_{ğ‘11}$ as normal distribution with mean at 1481 MPa and standard deviation at 54 MPa are close to the expected values., and the $ğ‘š_ğ‘$ was simulated as beta distribution with ğ›¼=112.61 , ğ›½=382.14 (equiv. to mean value at 0.22 and std at 0.018) which also represents a good results. The value of deviance is relatively high with $8268.584$. This value is an indicator for the stability of the model using the identified values.

## Insights and Conclusions

-   **Data Reduction Benefits:** The exploration of reducing the frequency and number of variables for information recording can lead to significant improvements in processing efficiency. By simplifying the dataset, we can enhance computational performance and reduce noise, allowing for more accurate modeling and clearer insights.

-   **Model Complexity and Performance:** While the Gibbs Sampler (JAGS model) has produced promising expected values, the complexity of the model can lead to challenges in interpretability and stability. Simplifying the model structure or using fewer variables may not only help reduce deviance but also improve convergence rates and make the results more robust.

## References

1.  Adaptive loggingmodule formonitoringapplicationsusingcontrol internaldigital drive signalsC. Brecher, T. Rudolf, 2009, ProductionEngineering3, 305-312

2.  Signalvorverarbeitung zur Anwendung steuerungsintegrierter ProzessÃ¼berwachungC.Brecher, T Rudolf , wt-online, 2009, S. 479-486

3.  Fundamentals of Modern Manufacturing; M. P. Groover, John Wiley & Sons, Inc. 2002

4.  Werkzeugmaschinen 3 Mechatronische Systeme, Vorschubantriebe, Prozessdiagnose M. Weck, C. Brecher; Springer, 2006

5.  JAGS Version4.3.0 usermanual, https://people.stat.sc.edu/hansont/stat740/jags_user_manual.pdf

6.  RegresiÃ³n Avanzada (con enfoque Bayesiano); L. E. Nieto Barajas, Scriptum maetriaen Ciencias de Datos, ITAM
